# Vision-Transformer-ViT
**Transformer** Architecture with **Attention** to classify the images.

## Description:
**Vision Transformer** is a model for *Image Classification*. An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.

![image](https://github.com/harishhirthi/Vision-Transformer-ViT/assets/43694283/3c9a6260-6852-4cad-9098-7755b984be75)

## Dataset:
[Animals Classification](https://www.kaggle.com/datasets/alessiocorrado99/animals10)

## References:
1. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).
2. [YouTube](https://youtu.be/Vonyoz6Yt9c?si=OgQ1oIfsJ9aq7uRO).
3. [YouTube](https://youtu.be/ovB0ddFtzzA?si=i_56yXqmPmK1vBNE).

## Contains:
* Python File - Class module for Vision Transformer Model
* Jupyter Notebook - Explains about Training and Inference
* Folder.


